计算机自然语言处理，王晓龙 关毅

## 第二章 数学基础
### 2.1 初等概率理论：probability theory

<br>大量重复试验或观察中所呈现出的股友规律，称为统计规律</br>

随机试验的所有可能结果或全体基本事件构成的集合称为<b>样本空间</b>，记做 <b>Ω</b> </br>

#### 2.1.2 条件概率于独立

P(A|B) = P(A ∩ B)   / P(B)              。。。 全概率公式

P(A ∩ B | C) = P(A|C) P(B|C)   

### 2.1.3 全概率公式与贝叶斯公式

P(A) = P(B<sub>1</sub>)P(A|B<sub>1</sub>) + P(B<sub>2</sub>)P(A| <sub>2</sub>) +
... + P(B<sub>n</sub>) P(A| B<sub>n</sub>)
= ∑ <sub>i=1</sub><sup>n</sup> P(B<sub>i</sub>) * P(A|B<sub>i</sub>)

P(B|A) = P(AB)/ P(A) = P(A|B) P(B) / P(A)  。。。  贝叶斯公式

argmax<sub>B</sub> P(B|A) = argmax<sub>B</sub> P(A|B)P(B) / P(A)
= argmax<sub>B</sub> P(A|B)P(B)


贝叶斯定理

P(B<sub>j</sub> | A) = P(A|B<sub>j</sub>) P(B<sub>j</sub>) / 
∑ <sup>n</sup><sub>i=1</sub>P(A|B<sub>i</sub>)P(B<sub>i</sub>)

#### 2.1.4 随机变量

* 离散型随机变量 ---- 分布函数 (distribution function)
* 连续型随机变量 ---- PDF,probablity density function or probability mass function, PMF 概率密度

#### 2.1.6 数学期望与方差

定义：设离散型随机变量ε 的分布律为：
P(ε = x<sub>k</sub>) = p<sub>k</sub>, k = 1, 2, ...
若级数 ∑ <sup>∞</sup> <sub>k=1</sub> x<sub>k</sub>p<sub>k</sub>绝对收敛，则称级数
 ∑ <sup>∞</sup> <sub>k=1</sub> x<sub>k</sub>p<sub>k</sub> 为随机变量ε的数学期望，即：
 E(ε) =  ∑ <sup>∞</sup> <sub>k=1</sub> x<sub>k</sub>p<sub>k</sub> 

设连续型随机变量ε的概率密度为f(x), 若积分∫<sub>-∞</sub><sup>+∞</sup> xf(x)dx 绝对收敛，则称
E (ε) = ∫<sub>-∞</sub><sup>+∞</sup> xf(x)dx 

<!--  
 \sum
 \infty 

 $$\sum$$ -->


#### 2.1.7 常用分布
1. 二项分布 (binomial distribution)

<b>离散</b>型随机变量 ε 只可能去两个值，即，当事件A不出现时，ε = 0, 当事件 A 出现时，ε = 1。这种只有两个可能
结果的随机试验成为伯努利试验(Bernoulli trials).

重复进行 n 次的独立的伯努利试验，这里“重复”指在每次试验中A(A补) 出现的概率不变，这种试验称为 n 重伯努利
  试验。

在n重伯努利试验中，设事件A出现的概率为p (0< p < 1), 以 ε 表示 n 次试验中事件 A 出现的次数， ε 的值可能
  为 0， 1， 2， ..., n. 令 P (ε = k ) = P <sub>k</sub> (n, p), 则：
  P<sub>k</sub> (n, p) = ( <sup> n </sup> <sub>k </sub>) p <sup>k</sub> (1-p)<sup>n-k</sup>,
  k = 0, 1,..., n

所以， ε 服从参数为 n 和 p 的二项分布，记做 ε ~ B (n, p)

二项分布的数学期望是 np, 方差是 np(1-p)

二项分布在统计语言英语语料库中，含 the 的语句占语料库中语句总数的比例近似服从二项分布。

2. 泊松分布 (poisson distribution)

离散型变量。。。。

泊松定理：设有二项分布 {B (n, p<sub>n</sub>)}, 其中参数列{p<sub>n</sub>} 满足 
lim <sub>n -> ∞ </sub> n p<sub>n</sub> =  λ > 0, 则对任意非负整数 k, 有

lim p<sub>k</sub>(n, p<sub>n</sub>) = e<sup>-λ</sup> * λ<sup>k</sup> / k!


3. 正态分布（normal distribution)
<b> 连续</b> 型随机变量最重要的概率分布是正态分布，又称高斯分布（Gaussian distribution)

### 2.2 信息论基础

#### 2.2.1 信息熵
不确定的程度是信息量的一个量度，不确定性越大，信息量就越大，反之则越小。

抛硬币和掷骰子的不确定性分别为：
<br> H(掷一次硬币) = f(2)</br>
<br> H(掷一次骰子) = f(6)</br>

再考虑随机试验两者同时进行的不确定性，则：
<br> H(抛一次硬币同时掷一次骰子) = f(12)</br>

不确定性与随机实验的可能结果的个数满足这个关系：
H = log k

在信息领域，底数通常取2。

在掷骰子的过程中，各种结果概率都相同，而在信息通信中，每个字的出现概率却并不一定相同，
而是具有特定的概率分布，令p(x)为一个在有限字符集 Ω 上取值的随机变量X的概率密度为：
p(x) = P(X = x), x ∈ Ω

随机变量的信息熵是该随机变量的平均不确定程度：
H(p) = H(X) = - ∑ p(x)pbp(x) = ∑ p(x)pb 1/p(x)

另一种解释是
H(X) = E (pb 1/p(x))

#### 2.2.2 联合熵和条件熵

#### 2.2.3 互信息

H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
于是，H(X) - H(X|Y) = H(Y) - H(Y|X)
这一差值称作随机变量X 和 Y的互信息(mutual information), 记作I (X; Y).
两个随机变量的互信息可以解释为知道一个随机变量的取值后对另一个随机变量的不确定性的
减少量，或者一个随机变量包含的另一个随机变量的信息量。

当两个随机变量互相独立时，他们的互信息刚好为0，互信息的取值越大，表明两个随机变量的
依赖程度越高。

I (X; Y) = ∑ <sub>x*y</sub> p(x, y) log(p(x,y)/p(x)p(y))

在自然语言词汇的搭配关系中，经常使用“互信息”作为描述两个单词之间关联程度大小的量度。
这里的“互信息”是指点与点的互信息，它的计算方法由下式给出：
I（x, y) = log p(x, y)/ p(x) p(y)

#### 2.2.4 相关熵
给定两个概率密度函数p(x)和q(x),它们的相关熵为：
D(p||q) = ∑<sub>x in omega</sub> p(x) *log p(x)/q(x)

相关熵又称为Kullback-Leibler距离，是衡量在同一事件空间的两个概率分布的差异的一个量度。
如果用数学期望来描述，则，
D(p||q) = E<sub>p</sub> (log p(X)/q(X))


## 第三章 汉语自动分词技术
字是汉语的最小单位。中文分词中的主要问题，包括汉语分词的规范、歧义的分类和识别，新词的识别
等问题。常见的分词方法：正向最大匹配分词方法、反向最大匹配分词方法、全切分词网络分词法等。

### 3.1 引言
1998年，国家863计划智能主题专家组的测评中，最高的分词正确率为：87.42%
汉语粉刺系统面临的挑战包括：
1. 如何识别未登录词。由于不存在绝对完备的词典，虽然一般的词典都能覆盖大多数词语，但有相当
一部分词语不可能穷尽的收入系统词典中，常见的未登录词有如下几类：
- 专有名词
- 重叠词
- 派生词。如，”一次性用品“
- 术语

2. 如何廉价搞笑地获取分词规则是汉语分词系统设计中不可忽视的问题之一。

3. 所谓词语边界歧义，指的是对于一个给定的汉语句子或汉字串，有多种词语边界划分形式。汉语边界歧义
包括组合歧义和交叉歧义。

4. 实时性问题。

### 3.2 分词规范
11条细规则

### 3.3 常用的分词方法
#### 3.3.1 正向最大匹配分词 （Forward maximum matching method, FMM)
假设自动分词词典中的最长词条所含汉字个数为I, 则被处理材料当前字符串叙述中的I 个字作为匹配字段，
查找分词词典。弱词典中有这样的一个I字词，则匹配成功，匹配字段作为一个词被切分出来。
如果词典找不到这样一个I字词，则匹配失败。匹配字段去掉最后一个汉字，剩下的字符作为新的匹配字段，进行
新的匹配，如此进行下去，直至切分成功为止。

错误率1/169

#### 3.3.2 反向最大匹配分词 （backward maximum matching method, BMM)
他的分词






