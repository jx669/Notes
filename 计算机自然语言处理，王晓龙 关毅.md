计算机自然语言处理，王晓龙 关毅

## 第二章 数学基础
### 2.1 初等概率理论：probability theory

<br>大量重复试验或观察中所呈现出的股友规律，称为统计规律</br>

随机试验的所有可能结果或全体基本事件构成的集合称为<b>样本空间</b>，记做 <b>Ω</b> </br>

#### 2.1.2 条件概率于独立

P(A|B) = P(A ∩ B)   / P(B)              。。。 全概率公式

P(A ∩ B | C) = P(A|C) P(B|C)   

### 2.1.3 全概率公式与贝叶斯公式

P(A) = P(B<sub>1</sub>)P(A|B<sub>1</sub>) + P(B<sub>2</sub>)P(A| <sub>2</sub>) +
... + P(B<sub>n</sub>) P(A| B<sub>n</sub>)
= ∑ <sub>i=1</sub><sup>n</sup> P(B<sub>i</sub>) * P(A|B<sub>i</sub>)

P(B|A) = P(AB)/ P(A) = P(A|B) P(B) / P(A)  。。。  贝叶斯公式

argmax<sub>B</sub> P(B|A) = argmax<sub>B</sub> P(A|B)P(B) / P(A)
= argmax<sub>B</sub> P(A|B)P(B)


贝叶斯定理

P(B<sub>j</sub> | A) = P(A|B<sub>j</sub>) P(B<sub>j</sub>) / 
∑ <sup>n</sup><sub>i=1</sub>P(A|B<sub>i</sub>)P(B<sub>i</sub>)

#### 2.1.4 随机变量

* 离散型随机变量 ---- 分布函数 (distribution function)
* 连续型随机变量 ---- PDF,probablity density function or probability mass function, PMF 概率密度

#### 2.1.6 数学期望与方差

定义：设离散型随机变量ε 的分布律为：
P(ε = x<sub>k</sub>) = p<sub>k</sub>, k = 1, 2, ...
若级数 ∑ <sup>∞</sup> <sub>k=1</sub> x<sub>k</sub>p<sub>k</sub>绝对收敛，则称级数
 ∑ <sup>∞</sup> <sub>k=1</sub> x<sub>k</sub>p<sub>k</sub> 为随机变量ε的数学期望，即：
 E(ε) =  ∑ <sup>∞</sup> <sub>k=1</sub> x<sub>k</sub>p<sub>k</sub> 

设连续型随机变量ε的概率密度为f(x), 若积分∫<sub>-∞</sub><sup>+∞</sup> xf(x)dx 绝对收敛，则称
E (ε) = ∫<sub>-∞</sub><sup>+∞</sup> xf(x)dx 

<!--  
 \sum
 \infty 

 $$\sum$$ -->


#### 2.1.7 常用分布
1. 二项分布 (binomial distribution)

<b>离散</b>型随机变量 ε 只可能去两个值，即，当事件A不出现时，ε = 0, 当事件 A 出现时，ε = 1。这种只有两个可能
结果的随机试验成为伯努利试验(Bernoulli trials).

重复进行 n 次的独立的伯努利试验，这里“重复”指在每次试验中A(A补) 出现的概率不变，这种试验称为 n 重伯努利
  试验。

在n重伯努利试验中，设事件A出现的概率为p (0< p < 1), 以 ε 表示 n 次试验中事件 A 出现的次数， ε 的值可能
  为 0， 1， 2， ..., n. 令 P (ε = k ) = P <sub>k</sub> (n, p), 则：
  P<sub>k</sub> (n, p) = ( <sup> n </sup> <sub>k </sub>) p <sup>k</sub> (1-p)<sup>n-k</sup>,
  k = 0, 1,..., n

所以， ε 服从参数为 n 和 p 的二项分布，记做 ε ~ B (n, p)

二项分布的数学期望是 np, 方差是 np(1-p)

二项分布在统计语言英语语料库中，含 the 的语句占语料库中语句总数的比例近似服从二项分布。

2. 泊松分布 (poisson distribution)

离散型变量。。。。

泊松定理：设有二项分布 {B (n, p<sub>n</sub>)}, 其中参数列{p<sub>n</sub>} 满足 
lim <sub>n -> ∞ </sub> n p<sub>n</sub> =  λ > 0, 则对任意非负整数 k, 有

lim p<sub>k</sub>(n, p<sub>n</sub>) = e<sup>-λ</sup> * λ<sup>k</sup> / k!


3. 正态分布（normal distribution)
<b> 连续</b> 型随机变量最重要的概率分布是正态分布，又称高斯分布（Gaussian distribution)

### 2.2 信息论基础

#### 2.2.1 信息熵
不确定的程度是信息量的一个量度，不确定性越大，信息量就越大，反之则越小。

抛硬币和掷骰子的不确定性分别为：
<br> H(掷一次硬币) = f(2)</br>
<br> H(掷一次骰子) = f(6)</br>

再考虑随机试验两者同时进行的不确定性，则：
<br> H(抛一次硬币同时掷一次骰子) = f(12)</br>

不确定性与随机实验的可能结果的个数满足这个关系：
H = log k

在信息领域，底数通常取2。

在掷骰子的过程中，各种结果概率都相同，而在信息通信中，每个字的出现概率却并不一定相同，
而是具有特定的概率分布，令p(x)为一个在有限字符集 Ω 上取值的随机变量X的概率密度为：
p(x) = P(X = x), x ∈ Ω






